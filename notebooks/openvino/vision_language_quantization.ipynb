{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaed3927-e315-46d3-8889-df3f3bbcbf6b",
   "metadata": {},
   "source": [
    "# Quantize your VLM with ðŸ¤— Optimum Intel\n",
    "\n",
    "This notebook shows how to quantize a question answering model with [Optimum Intel](https://huggingface.co/docs/optimum-intel/en/openvino/optimization) and OpenVINO's [Neural Network Compression Framework](https://github.com/openvinotoolkit/nncf) (NNCF). \n",
    "\n",
    "Quantization is a technique to reduce the computational and memory costs of running inference by representing the weights and / or the activations with lower precision data types like 8-bit or 4-bit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70eeef0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1: Installation and Setup\n",
    "\n",
    "First, let's install the required dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ebc847-8181-4c8a-9236-12cb23904773",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "If you're opening this Notebook on colab, you will probably need to install ðŸ¤— Optimum, . Uncomment the following cell and run it.\n",
    " First make sure everything is installed as expected by uncommenting this cell :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffab375-a730-4015-8d17-360b76a0718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"optimum-intel[openvino]\" datasets num2words\n",
    "! pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a179812",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 2: Preparation\n",
    "\n",
    "Now let's load the processor and prepare our input data. We'll use a sample image of a bee on a flower and ask the model what's on the flower.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860ff939",
   "metadata": {},
   "source": [
    "![image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f253327b-af28-41de-b010-8edbec3c2c4a",
   "metadata": {},
   "source": [
    "Load processor and prepare inputs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ff192-1b1e-4cec-ab83-119faf494c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoProcessor\n",
    "from transformers.image_utils import load_image\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "model_id = \"echarlaix/SmolVLM2-256M-Video-Instruct-openvino\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "prompt, img_url = \"What is on the flower?\", \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prepare inputs\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(text=prompt, images=[load_image(img_url)], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9c5734",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 3: Load Original Model and Test\n",
    "\n",
    "Let's load the original FP32 model and test it with our prepared inputs to establish a baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e776d77-b19c-4a82-a7ba-026143ab2035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel import OVModelForVisualCausalLM\n",
    "\n",
    "\n",
    "model_ov = OVModelForVisualCausalLM.from_pretrained(model_id, load_in_8bit=False)\n",
    "fp32_model_path = \"smolvlm_ov\"\n",
    "model_ov.save_pretrained(fp32_model_path)\n",
    "\n",
    "# Generate outputs\n",
    "generated_ids = model_ov.generate(**inputs, max_new_tokens=500)\n",
    "generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(generated_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1075a71e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 4: Configure and Apply Quantization\n",
    "\n",
    "Now we'll configure the quantization settings and apply them to create a quantized version of our model. You can explore other quantization options [here](https://huggingface.co/docs/optimum/en/intel/openvino/optimization) and by playing with the different quantization configurations defined below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd08433",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Step 4a: Configure Quantization Settings\n",
    "\n",
    "To apply quantization on your model you need to create a quantization configuration specifying the methodology to use. By default 8bit weight-only quantization will be applied on the text and vision embeddings components, while the language model will be quantized depending on the specified quantization configuration `quantization_config`. A specific quantization configuration can be defined for each components as well, this can be done by creating an instance of `OVPipelineQuantizationConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccb1914-d64e-4daf-b274-b8979e427a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel import OVQuantizationConfig, OVWeightQuantizationConfig, OVPipelineQuantizationConfig\n",
    "\n",
    "dataset, num_samples = \"contextual\", 50\n",
    "\n",
    "# weight-only 8bit\n",
    "woq_8bit = OVWeightQuantizationConfig(bits=8)\n",
    "\n",
    "# weight-only 4bit\n",
    "woq_4bit = OVWeightQuantizationConfig(bits=4, group_size=16)\n",
    "\n",
    "# static quantization\n",
    "static_8bit = OVQuantizationConfig(bits=8, dataset=dataset, num_samples=num_samples)\n",
    "\n",
    "# pipeline quantization: applying different quantization on each components\n",
    "ppl_q = OVPipelineQuantizationConfig(\n",
    "    quantization_configs={\n",
    "        \"lm_model\": OVQuantizationConfig(bits=8),\n",
    "        \"text_embeddings_model\": OVWeightQuantizationConfig(bits=8),\n",
    "        \"vision_embeddings_model\": OVWeightQuantizationConfig(bits=8),\n",
    "    },\n",
    "    dataset=dataset,\n",
    "    num_samples=num_samples,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e159efa8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Step 4b: Apply Quantization\n",
    "\n",
    "You can now apply quantization on your model, here we apply wieght-only quantization on our model defined in `woq_8bit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56c799d-2e0c-49e3-9698-ae0a92a80150",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_model = OVModelForVisualCausalLM.from_pretrained(model_id, quantization_config=woq_8bit)\n",
    "int8_model_path = \"smolvlm_int8\"\n",
    "q_model.save_pretrained(int8_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0558b3b8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: Compare Results\n",
    "\n",
    "Let's test the quantized model and compare it with the original model in terms of both output quality and model size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52faa10",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Step 5a: Test Quantized Model Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc85d61-010b-4ea4-83a9-21391bc43cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate outputs with quantized model\n",
    "generated_ids = q_model.generate(**inputs, max_new_tokens=500)\n",
    "generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(generated_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7778bf",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Step 5b: Compare Model Sizes\n",
    "\n",
    "Now let's compare the file sizes of the original FP32 model and the quantized INT8 model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeaa81f-7fc5-49ba-80b8-2d95a1310a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def get_model_size(model_folder):\n",
    "    model_size = 0\n",
    "    for file in Path(model_folder).iterdir():\n",
    "        if file.suffix==\".xml\":\n",
    "            model_size += file.stat().st_size + file.with_suffix(\".bin\").stat().st_size\n",
    "    model_size /= 1000 * 1000\n",
    "    return model_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c862277",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_model_size = get_model_size(fp32_model_path)\n",
    "int8_model_size = get_model_size(int8_model_path)\n",
    "print(f\"FP32 model size: {fp32_model_size:.2f} MB\")\n",
    "print(f\"INT8 model size: {int8_model_size:.2f} MB\")\n",
    "print(f\"INT8 size decrease: {fp32_model_size / int8_model_size:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef9297",
   "metadata": {},
   "source": [
    "### 5c: Compare performance on different Intel Hardware platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f18ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from optimum.intel import OVModelForVisualCausalLM\n",
    "\n",
    "class InferRequestWrapper:\n",
    "    \"\"\"\n",
    "    A helper class to track pipeline components' inference time.\n",
    "    \"\"\"\n",
    "    def __init__(self, request, infer_time_values):\n",
    "        self.request = request\n",
    "        self.infer_time_values = infer_time_values\n",
    "        self._start_async_time = None\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.request.reset_state()\n",
    "\n",
    "    def get_tensor(self, name):\n",
    "        return self.request.get_tensor(name)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        start_time = time.perf_counter()\n",
    "        result = self.request(*args, **kwargs)\n",
    "        end_time = time.perf_counter()\n",
    "        self.infer_time_values.append(end_time - start_time)\n",
    "        return result\n",
    "\n",
    "    def start_async(self, *args, **kwargs):\n",
    "        assert self._start_async_time is None, \"start_async is already in progress\"\n",
    "        self._start_async_time = time.perf_counter()\n",
    "        return self.request.start_async(*args, **kwargs)\n",
    "\n",
    "    def wait(self):\n",
    "        assert self._start_async_time is not None, \"start_async must be called before wait\"\n",
    "        result = self.request.wait()\n",
    "        self.infer_time_values.append(time.perf_counter() - self._start_async_time)\n",
    "        self._start_async_time = None\n",
    "        return result\n",
    "\n",
    "\n",
    "def benchmark(model, inputs, model_dir: str, nb_pass=10, warmup=4,max_tokens=50):\n",
    "    \"\"\"\n",
    "    Benchmark an OV visual causal LM model.\n",
    "\n",
    "    Returns a dict with:\n",
    "    - avg_latency_sec\n",
    "    - image_throughput\n",
    "    - first_token_throughput\n",
    "    - second_token_throughput\n",
    "    - model_size_mb\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Patch OpenVINO InferRequest objects to track inference time ---\n",
    "    model.compile()\n",
    "    lm_model_time_values = []\n",
    "    vision_embed_time_values = []\n",
    "    first_token_latencies = []\n",
    "    model.language_model.request = InferRequestWrapper(model.language_model.request, lm_model_time_values)\n",
    "    model.vision_embeddings.request = InferRequestWrapper(model.vision_embeddings.request, vision_embed_time_values)\n",
    "    \n",
    "    # --- Warmup ---\n",
    "    for _ in range(warmup):\n",
    "        _ = model.generate(**inputs)\n",
    "\n",
    "    lm_model_time_values.clear()\n",
    "    vision_embed_time_values.clear()\n",
    "\n",
    "    # --- Timed inference ---\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(nb_pass):\n",
    "        last_infer_count = len(lm_model_time_values)\n",
    "        outputs = model.generate(**inputs,max_new_tokens=max_tokens)\n",
    "        first_token_latencies.append(lm_model_time_values[last_infer_count])\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    # --- Unpatch InferRequest objects ---\n",
    "    model.language_model.request = model.language_model.request.request\n",
    "    model.vision_embeddings.request = model.vision_embeddings.request.request\n",
    "\n",
    "    # --- Throughput calculations ---\n",
    "    avg_latency = (end - start) / nb_pass\n",
    "    \n",
    "    avg_vision_embed_time = sum(vision_embed_time_values) / len(vision_embed_time_values)\n",
    "    avg_first_token_latency = sum(first_token_latencies) / len(first_token_latencies)\n",
    "    avg_second_token_latency = (sum(lm_model_time_values) - sum(first_token_latencies)) / \\\n",
    "        (len(lm_model_time_values) - len(first_token_latencies))\n",
    "\n",
    "    batch_size = inputs[\"pixel_values\"].shape[0] if \"pixel_values\" in inputs else 1\n",
    "    image_throughput = batch_size / avg_vision_embed_time\n",
    "\n",
    "    # --- Model size ---\n",
    "    model_size_bytes = sum(\n",
    "        os.path.getsize(os.path.join(model_dir, f))\n",
    "        for f in os.listdir(model_dir)\n",
    "        if f.startswith(\"openvino_\")\n",
    "    )\n",
    "    model_size_mb = model_size_bytes / (1024**2)\n",
    "\n",
    "    return {\n",
    "        \"avg_latency_sec\": avg_latency,\n",
    "        \"image_throughput\": image_throughput,\n",
    "        \"first_token_throughput\": 1 / avg_first_token_latency,\n",
    "        \"second_token_throughput\": 1 / avg_second_token_latency,\n",
    "        \"model_size_mb\": model_size_mb,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f111381d",
   "metadata": {},
   "source": [
    "#### Run benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for available hardware platforms\n",
    "\n",
    "from openvino.runtime import Core\n",
    "\n",
    "core = Core()\n",
    "devices = core.available_devices\n",
    "device_list = []\n",
    "\n",
    "for device in devices:\n",
    "    try:\n",
    "        # Use FULL_DEVICE_NAME if available, else fallback to device ID\n",
    "        name = core.get_property(device, \"FULL_DEVICE_NAME\")\n",
    "    except:\n",
    "        name = device\n",
    "    device_list.append(device)  # keep the device ID for model loading\n",
    "    print(f\"{device}: {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b8341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Local models ---\n",
    "models = {\n",
    "    \"SmolVLM2-256M (full)\": fp32_model_path,\n",
    "    \"SmolVLM2-256M-int8\": int8_model_path\n",
    "}\n",
    "\n",
    "# --- Run benchmark ---\n",
    "for model_name, model_dir in models.items():\n",
    "    for device in device_list:\n",
    "        print(f\"\\nBenchmarking {model_name} on {device}...\")\n",
    "\n",
    "        # Load model for the specific device\n",
    "        model_ov = OVModelForVisualCausalLM.from_pretrained(\n",
    "            model_dir, export=False, device=device\n",
    "        )\n",
    "\n",
    "        # Run benchmark\n",
    "        results = benchmark(model_ov, inputs, model_dir=model_dir)\n",
    "\n",
    "        # Print results\n",
    "        print(\n",
    "            f\"Latency: {results['avg_latency_sec']:.4f}s | \"\n",
    "            f\"Image throughput: {results['image_throughput']:.2f} im/s | \"\n",
    "            f\"First token throughput: {results['first_token_throughput']:.2f} t/s | \"\n",
    "            f\"Second token throughput: {results['second_token_throughput']:.2f} t/s | \"\n",
    "            f\"Model size: {results['model_size_mb']:.2f} MB\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43531db0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Great! We've successfully quantized our VLM model using Optimum Intel. The results show:\n",
    "\n",
    "1. **Quality**: The quantized model produces the same output as the original model\n",
    "2. **Size**: We achieved approximately 4x reduction in model size (from ~1GB to ~260MB)\n",
    "3. **Performance**: The INT8 model has been reduced on size maintaining the accuracy\n",
    "\n",
    "This demonstrates how quantization can significantly reduce model size preserving the model's accuracy for visual language tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
