{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaed3927-e315-46d3-8889-df3f3bbcbf6b",
   "metadata": {},
   "source": [
    "# Quantize your VLM with 🤗 Optimum Intel\n",
    "\n",
    "This notebook shows how to quantize a question answering model with [Optimum Intel](https://huggingface.co/docs/optimum-intel/en/openvino/optimization) and OpenVINO's [Neural Network Compression Framework](https://github.com/openvinotoolkit/nncf) (NNCF). \n",
    "\n",
    "Quantization is a technique to reduce the computational and memory costs of running inference by representing the weights and / or the activations with lower precision data types like 8-bit or 4-bit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70eeef0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1: Installation and Setup\n",
    "\n",
    "First, let's install the required dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ebc847-8181-4c8a-9236-12cb23904773",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "If you're opening this Notebook on colab, you will probably need to install 🤗 Optimum, . Uncomment the following cell and run it.\n",
    " First make sure everything is installed as expected by uncommenting this cell :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dffab375-a730-4015-8d17-360b76a0718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install \"optimum-intel[openvino]\" datasets num2words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a179812",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 2: Preparation\n",
    "\n",
    "Now let's load the processor and prepare our input data. We'll use a sample image of a bee on a flower and ask the model what's on the flower.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860ff939",
   "metadata": {},
   "source": [
    "![image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f253327b-af28-41de-b010-8edbec3c2c4a",
   "metadata": {},
   "source": [
    "Load processor and prepare inputs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee1ff192-1b1e-4cec-ab83-119faf494c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoProcessor\n",
    "from transformers.image_utils import load_image\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "model_id = \"echarlaix/SmolVLM2-256M-Video-Instruct-openvino\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "prompt, img_url = \"What is on the flower?\", \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prepare inputs\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(text=prompt, images=[load_image(img_url)], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9c5734",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 3: Load Original Model and Test\n",
    "\n",
    "Let's load the original FP32 model and test it with our prepared inputs to establish a baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e776d77-b19c-4a82-a7ba-026143ab2035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:\n",
      "\n",
      "\n",
      "\n",
      "What is on the flower?\n",
      "Assistant: A bee is on the flower.\n"
     ]
    }
   ],
   "source": [
    "from optimum.intel import OVModelForVisualCausalLM\n",
    "\n",
    "\n",
    "model_ov = OVModelForVisualCausalLM.from_pretrained(model_id, load_in_8bit=False)\n",
    "fp32_model_path = \"smolvlm_ov\"\n",
    "model_ov.save_pretrained(fp32_model_path)\n",
    "\n",
    "# Generate outputs\n",
    "generated_ids = model_ov.generate(**inputs, max_new_tokens=500)\n",
    "generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(generated_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1075a71e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 4: Configure and Apply Quantization\n",
    "\n",
    "Now we'll configure the quantization settings and apply them to create a quantized version of our model. You can explore other quantization options [here](https://huggingface.co/docs/optimum/en/intel/openvino/optimization) and by playing with the different quantization configurations defined below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd08433",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Step 4a: Configure Quantization Settings\n",
    "\n",
    "To apply quantization on your model you need to create a quantization configuration specifying the methodology to use. By default 8bit weight-only quantization will be applied on the text and vision embeddings components, while the language model will be quantized depending on the specified quantization configuration `quantization_config`. A specific quantization configuration can be defined for each components as well, this can be done by creating an instance of `OVPipelineQuantizationConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ccb1914-d64e-4daf-b274-b8979e427a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel import OVQuantizationConfig, OVWeightQuantizationConfig, OVPipelineQuantizationConfig\n",
    "\n",
    "dataset, num_samples = \"contextual\", 50\n",
    "\n",
    "# weight-only 8bit\n",
    "woq_8bit = OVWeightQuantizationConfig(bits=8)\n",
    "\n",
    "# weight-only 4bit\n",
    "woq_4bit = OVWeightQuantizationConfig(bits=4, group_size=16)\n",
    "\n",
    "# static quantization\n",
    "static_8bit = OVQuantizationConfig(bits=8, dataset=dataset, num_samples=num_samples)\n",
    "\n",
    "# pipeline quantization: applying different quantization on each components\n",
    "ppl_q = OVPipelineQuantizationConfig(\n",
    "    quantization_configs={\n",
    "        \"lm_model\": OVQuantizationConfig(bits=8),\n",
    "        \"text_embeddings_model\": OVWeightQuantizationConfig(bits=8),\n",
    "        \"vision_embeddings_model\": OVWeightQuantizationConfig(bits=8),\n",
    "    },\n",
    "    dataset=dataset,\n",
    "    num_samples=num_samples,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e159efa8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Step 4b: Apply Quantization\n",
    "\n",
    "You can now apply quantization on your model, here we apply wieght-only quantization on our model defined in `woq_8bit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f56c799d-2e0c-49e3-9698-ae0a92a80150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "┍━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑\n",
      "│ Weight compression mode   │ % all parameters (layers)   │ % ratio-defining parameters (layers)   │\n",
      "┝━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥\n",
      "│ int8_asym                 │ 100% (211 / 211)            │ 100% (211 / 211)                       │\n",
      "┕━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/eze/optimum-intel/notebooks/openvino/ov/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/eze/optimum-intel/notebooks/openvino/ov/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "┍━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑\n",
      "│ Weight compression mode   │ % all parameters (layers)   │ % ratio-defining parameters (layers)   │\n",
      "┝━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥\n",
      "│ int8_sym                  │ 100% (1 / 1)                │ 100% (1 / 1)                           │\n",
      "┕━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "┍━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑\n",
      "│ Weight compression mode   │ % all parameters (layers)   │ % ratio-defining parameters (layers)   │\n",
      "┝━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥\n",
      "│ int8_sym                  │ 100% (75 / 75)              │ 100% (75 / 75)                         │\n",
      "┕━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "┍━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑\n",
      "│ Weight compression mode   │ % all parameters (layers)   │ % ratio-defining parameters (layers)   │\n",
      "┝━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥\n",
      "│ int8_sym                  │ 100% (75 / 75)              │ 100% (75 / 75)                         │\n",
      "┕━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "q_model = OVModelForVisualCausalLM.from_pretrained(model_id, quantization_config=woq_8bit)\n",
    "int8_model_path = \"smolvlm_int8\"\n",
    "q_model.save_pretrained(int8_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cf1523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size: 248.86 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total size: {get_model_size(base=\"smolvlm_int8\") / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26883f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Inference latency: 2.1692 seconds\n"
     ]
    }
   ],
   "source": [
    "# 2 rounds of warm-up and 5 rounds of inference (to measure the average)\n",
    "avg_latency = elapsed_time(q_model, inputs, nb_pass=5, warmup=2)\n",
    "print(f\"Average Inference latency: {avg_latency:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0558b3b8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: Compare Results\n",
    "\n",
    "Let's test the quantized model and compare it with the original model in terms of both output quality and model size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52faa10",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Step 5a: Test Quantized Model Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc85d61-010b-4ea4-83a9-21391bc43cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:\n",
      "\n",
      "\n",
      "\n",
      "What is on the flower?\n",
      "Assistant: A bee is on the flower.\n"
     ]
    }
   ],
   "source": [
    "# Generate outputs with quantized model\n",
    "generated_ids = q_model.generate(**inputs, max_new_tokens=500)\n",
    "generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(generated_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7778bf",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Step 5b: Compare Model Sizes\n",
    "\n",
    "Now let's compare the file sizes of the original FP32 model and the quantized INT8 model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eeaa81f-7fc5-49ba-80b8-2d95a1310a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def get_model_size(model_folder):\n",
    "    model_size = 0\n",
    "    for file in Path(model_folder).iterdir():\n",
    "        if file.suffix==\".xml\":\n",
    "            model_size += file.stat().st_size + file.with_suffix(\".bin\").stat().st_size\n",
    "    model_size /= 1000 * 1000\n",
    "    return model_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c862277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 model size: 1028.25 MB\n",
      "INT8 model size: 260.94 MB\n",
      "INT8 size decrease: 3.94x\n"
     ]
    }
   ],
   "source": [
    "fp32_model_size = get_model_size(fp32_model_path)\n",
    "int8_model_size = get_model_size(int8_model_path)\n",
    "print(f\"FP32 model size: {fp32_model_size:.2f} MB\")\n",
    "print(f\"INT8 model size: {int8_model_size:.2f} MB\")\n",
    "print(f\"INT8 size decrease: {fp32_model_size / int8_model_size:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef9297",
   "metadata": {},
   "source": [
    "### 5c: Compare performance on different Intel Hardware platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f18ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from optimum.intel import OVModelForVisualCausalLM\n",
    "\n",
    "def benchmark(model, inputs, model_dir: str, nb_pass=10, warmup=4,max_tokens=50):\n",
    "    \"\"\"\n",
    "    Benchmark an OV visual causal LM model.\n",
    "\n",
    "    Returns a dict with:\n",
    "    - avg_latency_sec\n",
    "    - images_per_sec\n",
    "    - tokens_per_sec\n",
    "    - model_size_mb\n",
    "    \"\"\"\n",
    "    # --- Warmup ---\n",
    "    for _ in range(warmup):\n",
    "        _ = model.generate(**inputs)\n",
    "\n",
    "    # --- Timed inference ---\n",
    "    start = time.time()\n",
    "    for _ in range(nb_pass):\n",
    "        outputs = model.generate(**inputs,max_new_tokens=max_tokens)\n",
    "    end = time.time()\n",
    "\n",
    "    avg_latency = (end - start) / nb_pass\n",
    "\n",
    "    # --- Throughput calculations ---\n",
    "    batch_size = inputs[\"pixel_values\"].shape[0] if \"pixel_values\" in inputs else 1\n",
    "    num_tokens = outputs.shape[-1]  # sequence length\n",
    "\n",
    "    images_per_sec = batch_size / avg_latency\n",
    "    tokens_per_sec = num_tokens / avg_latency\n",
    "\n",
    "    # --- Model size ---\n",
    "    model_size_bytes = sum(\n",
    "        os.path.getsize(os.path.join(model_dir, f))\n",
    "        for f in os.listdir(model_dir)\n",
    "        if f.startswith(\"openvino_\")\n",
    "    )\n",
    "    model_size_mb = model_size_bytes / (1024**2)\n",
    "\n",
    "    return {\n",
    "        \"avg_latency_sec\": avg_latency,\n",
    "        \"images_per_sec\": images_per_sec,\n",
    "        \"tokens_per_sec\": tokens_per_sec,\n",
    "        \"model_size_mb\": model_size_mb,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f111381d",
   "metadata": {},
   "source": [
    "#### Run benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e558d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: Intel(R) Core(TM) Ultra 7 265K\n",
      "GPU.0: Intel(R) Graphics (iGPU)\n",
      "GPU.1: Intel(R) Arc(TM) B580 Graphics (dGPU)\n",
      "GPU.2: Intel(R) Arc(TM) A770 Graphics (dGPU)\n",
      "NPU: Intel(R) AI Boost\n"
     ]
    }
   ],
   "source": [
    "#Check for available hardware platforms\n",
    "\n",
    "from openvino.runtime import Core\n",
    "\n",
    "core = Core()\n",
    "devices = core.available_devices\n",
    "\n",
    "for device in devices:\n",
    "    try:\n",
    "        # Try to get the full device name property\n",
    "        name = core.get_property(device, \"FULL_DEVICE_NAME\")\n",
    "    except:\n",
    "        # If the property is not available, just use the device ID\n",
    "        name = device\n",
    "    print(f\"{device}: {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d5b8341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking SmolVLM2-256M (full) on CPU...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 3.5740s | Images/sec: 0.28 | Tokens/sec: 247.34 | Model size: 980.61 MB\n",
      "\n",
      "Benchmarking SmolVLM2-256M (full) on GPU.0...\n",
      "Latency: 2.1468s | Images/sec: 0.47 | Tokens/sec: 411.78 | Model size: 980.61 MB\n",
      "\n",
      "Benchmarking SmolVLM2-256M (full) on GPU.1...\n",
      "Latency: 0.2025s | Images/sec: 4.94 | Tokens/sec: 4364.79 | Model size: 980.61 MB\n",
      "\n",
      "Benchmarking SmolVLM2-256M (full) on GPU.2...\n",
      "Latency: 0.3380s | Images/sec: 2.96 | Tokens/sec: 2615.45 | Model size: 980.61 MB\n",
      "\n",
      "Benchmarking SmolVLM2-256M-int8 on CPU...\n",
      "Latency: 2.1794s | Images/sec: 0.46 | Tokens/sec: 405.61 | Model size: 248.86 MB\n",
      "\n",
      "Benchmarking SmolVLM2-256M-int8 on GPU.0...\n",
      "Latency: 2.6433s | Images/sec: 0.38 | Tokens/sec: 338.96 | Model size: 248.86 MB\n",
      "\n",
      "Benchmarking SmolVLM2-256M-int8 on GPU.1...\n",
      "Latency: 0.2289s | Images/sec: 4.37 | Tokens/sec: 3861.41 | Model size: 248.86 MB\n",
      "\n",
      "Benchmarking SmolVLM2-256M-int8 on GPU.2...\n",
      "Latency: 0.3563s | Images/sec: 2.81 | Tokens/sec: 2481.35 | Model size: 248.86 MB\n"
     ]
    }
   ],
   "source": [
    "# --- Local models ---\n",
    "models = {\n",
    "    \"SmolVLM2-256M (full)\": fp32_model_path,\n",
    "    \"SmolVLM2-256M-int8\": int8_model_path\n",
    "}\n",
    "devices = [\"CPU\", \"GPU.0\",\"GPU.1\",\"GPU.2\"]\n",
    "\n",
    "# --- Run benchmark ---\n",
    "for model_name, model_dir in models.items():\n",
    "    for device in devices:\n",
    "        print(f\"\\nBenchmarking {model_name} on {device}...\")\n",
    "        model_ov = OVModelForVisualCausalLM.from_pretrained(model_dir, export=False, device=device)\n",
    "        results = benchmark(model_ov, inputs, model_dir=model_dir)\n",
    "        print(f\"Latency: {results['avg_latency_sec']:.4f}s | Images/sec: {results['images_per_sec']:.2f} | Tokens/sec: {results['tokens_per_sec']:.2f} | Model size: {results['model_size_mb']:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43531db0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Great! We've successfully quantized our VLM model using Optimum Intel. The results show:\n",
    "\n",
    "1. **Quality**: The quantized model produces the same output as the original model\n",
    "2. **Size**: We achieved approximately 4x reduction in model size (from ~1GB to ~260MB)\n",
    "3. **Performance**: The INT8 model has been reduced on size maintaining the accuracy\n",
    "\n",
    "This demonstrates how quantization can significantly reduce model size preserving the model's accuracy for visual language tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ov",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
