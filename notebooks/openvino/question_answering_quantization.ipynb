{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaed3927-e315-46d3-8889-df3f3bbcbf6b",
   "metadata": {},
   "source": [
    "# Quantize a Hugging Face Question-Answering Model with OpenVINO\n",
    "\n",
    "This notebook shows how to quantize a question answering model with OpenVINO's Neural Network Compression Framework (NNCF). Question-answering models can understand and answer questions based on a given context, such as a paragraph of text or a document. \n",
    "\n",
    "OpenVINO is a toolkit for accelerated inference on Intel hardware, including CPUs and integrated GPUs. This allows developers to deploy Hugging Face's NLP models in a wide range of scenarios, from small edge devices to large cloud environments.\n",
    "\n",
    "To install the requirements for using this notebook, please do `pip install optimum[openvino,nncf] datasets evaluate[evaluator]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0407fc92-c052-47b7-8721-01836adf3b54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/venvs/optimum_pre_env/lib/python3.8/site-packages/openvino/offline_transformations/__init__.py:10: FutureWarning: The module is private and following namespace `offline_transformations` will be removed in the future, use `openvino.runtime.passes` instead!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from evaluate import evaluator\n",
    "from optimum.intel.openvino import OVConfig, OVModelForQuestionAnswering, OVQuantizer\n",
    "from transformers import (\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "from utils.trainer_qa import QuestionAnsweringTrainer\n",
    "from utils.utils_qa import (\n",
    "    post_processing_function_qa,\n",
    "    prepare_train_features,\n",
    "    prepare_validation_features,\n",
    ")\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "datasets.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889a16fe-2bc0-477e-b8d6-02a4f7508f03",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "We define MODEL_ID and DATASET_NAME, and the paths for the quantized model files. VERSION_2_WITH_NEGATIVE should be set to TRUE if a version of the SQuAD v2 dataset is used, which includes questions that do not have an answer. \n",
    "\n",
    "For this tutorial, we use the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad), a reading comprehension dataset, consisting of questions on a set of Wikipedia articles, where the answer to every question is a segment of text from a given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c32f9a76-414b-43d9-9769-af131223f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"csarron/bert-base-uncased-squad-v1\"\n",
    "# When using a different dataset then SQuAD v1, please edit the constants at the top of utils/utils_qa.py\n",
    "DATASET_NAME = \"squad\"\n",
    "VERSION_2_WITH_NEGATIVE = False\n",
    "\n",
    "base_model_path = Path(f\"models/{MODEL_ID}\")\n",
    "fp32_model_path = base_model_path.with_name(base_model_path.name + \"_FP32\")\n",
    "int8_ptq_model_path = base_model_path.with_name(base_model_path.name + \"_INT8_PTQ\")\n",
    "int8_qat_model_path = base_model_path.with_name(base_model_path.name + \"_INT8_QAT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3547df-5603-41b4-8752-df5fb7347be0",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer\n",
    "\n",
    "We load the model from the Hugging Face Hub. The model will be automatically downloaded if it has not been downloaded before, or loaded from the cache otherwise.\n",
    "\n",
    "We also load the tokenizer, which converts the questions and contexts from the dataset to tokens: numerical values in the format the model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38641b14-07d0-49d5-af86-8b5247ae39d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 7592, 2088, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_ID)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# See how the tokenizer converts input text to model input values\n",
    "print(tokenizer(\"hello world!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124bd9ad-077c-4f41-b579-0bf978fe6a1e",
   "metadata": {},
   "source": [
    "## Preview the Dataset\n",
    "\n",
    "The `datasets` library makes it easy to load datasets. Common datasets can be loaded from the Hugging Face Hub by providing the name of the dataset. See https://github.com/huggingface/datasets. We can load the SQuAD dataset with `load_dataset` and show a random dataset item. Every dataset item in the SQuAD dataset has a unique id, a title which denotes the category, a context and a question, and answers. The answer is a subset of the context, and both the text of the answer, and the start position of the answer in the context (answer_start) are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "602fe46f-c96a-4a0f-9338-58339d466f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef8d9c86f14e499d8e4802027a9a0e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': '570e53690b85d914000d7e3c',\n",
       " 'title': 'Melbourne',\n",
       " 'context': \"Melbourne is experiencing high population growth, generating high demand for housing. This housing boom has increased house prices and rents, as well as the availability of all types of housing. Subdivision regularly occurs in the outer areas of Melbourne, with numerous developers offering house and land packages. However, after 10 years[when?] of planning policies to encourage medium-density and high-density development in existing areas with greater access to public transport and other services, Melbourne's middle and outer-ring suburbs have seen significant brownfields redevelopment.\",\n",
       " 'question': 'What effect has the housing boom had on house prices and rents?',\n",
       " 'answers': {'text': ['increased'], 'answer_start': [108]}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(DATASET_NAME)\n",
    "dataset[\"train\"][31415]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4dfc2d-f007-4455-8043-edc5468b87e2",
   "metadata": {},
   "source": [
    "## Post Training Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5483309f-0b12-4c10-ab0b-58c50981f495",
   "metadata": {
    "tags": []
   },
   "source": [
    "For post-training quantization (PTQ) we start with a Hugging Face AutoModel, in this case AutoModelForQuestionAnswering. The quantizer also needs a dataset. \n",
    "\n",
    "To quantize a model with post-training quantization, we define an `OVQuantizer`, attach a dataset, and call the `quantize` method. That's all!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b14899a-3aec-46b6-9d4b-332452e9cb25",
   "metadata": {},
   "source": [
    "### Prepare the Dataset\n",
    "\n",
    "We need a representative dataset to quantize the model. The SQuAD dataset is pretrained on a large dataset with a wide variety of questions and answers, and it generalizes pretty well to questions and contexts it has never seen before. For production use, you would finetune this dataset with questions and context specific to your domain. In this notebook, we use a subset of the SQuAD dataset, for demonstration purposes. We chose the _Super Bowl 50_ category from the validation subset of SQuAD because it has a large number of questions.\n",
    "\n",
    "Post-training quantization does not need a training and validation dataset, but we define these splits here to allow doing quantization-aware training later in this notebook, and to make sure we're using the same dataset split for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8be68958-ce5e-4cc6-b8e7-2867feaf084b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_fn(examples, tokenizer):\n",
    "    return tokenizer(examples[\"question\"], examples[\"context\"], padding=True, truncation=True, max_length=384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b4dde1a-1ba6-470c-afe8-7169c04282dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN_ITEMS = 600\n",
    "filtered_examples = dataset[\"validation\"].filter(lambda x: x[\"title\"].startswith(\"Super_Bowl_50\"))\n",
    "train_examples = filtered_examples.select(range(0, NUM_TRAIN_ITEMS))\n",
    "train_dataset = train_examples.map(lambda x: preprocess_fn(x, tokenizer), batched=True)\n",
    "\n",
    "validation_examples = filtered_examples.select(range(NUM_TRAIN_ITEMS, len(filtered_examples)))\n",
    "validation_dataset = validation_examples.map(lambda x: preprocess_fn(x, tokenizer), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0b0738-fdc9-4557-97bf-b4c6709280cc",
   "metadata": {},
   "source": [
    "### Quantize the Model with Post Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31c5415e-e22b-4ab9-b903-8791e80b188d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hide PyTorch warnings about missing shape inference\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Quantize the model\n",
    "quantizer = OVQuantizer.from_pretrained(model)\n",
    "quantizer.quantize(calibration_dataset=train_dataset, save_directory=int8_ptq_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a52092-e352-47ef-9ed2-89508bc48d70",
   "metadata": {},
   "source": [
    "### Show accuracy difference\n",
    "\n",
    "We load the quantized model and the original FP32 model, and compare the metrics on both models. The [evaluate](https://github.com/huggingface/evaluate) library makes it very easy to evaluate models on a given dataset, with a given metric. For the SQuAD dataset, an F1 score and exact_match metric are returned.\n",
    "\n",
    "For loading the quantized model with OpenVINO, we use `OVModelForQuestionAnswering`. It can be used in the same way as [`AutoModelForQuestionAnswering`](https://huggingface.co/docs/transformers/main/model_doc/auto).\n",
    "\n",
    "The evaluator is called with a [Pipeline](https://huggingface.co/docs/transformers/main/en/pipeline_tutorial) which we will also use later on to show inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c2f615a-19e3-4ee2-9309-2ae1392c7f62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exact_match</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FP32</th>\n",
       "      <td>82.86</td>\n",
       "      <td>86.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INT8 PTQ</th>\n",
       "      <td>82.86</td>\n",
       "      <td>87.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          exact_match     f1\n",
       "FP32            82.86  86.33\n",
       "INT8 PTQ        82.86  87.42"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model_ptq = OVModelForQuestionAnswering.from_pretrained(int8_ptq_model_path)\n",
    "original_model = AutoModelForQuestionAnswering.from_pretrained(MODEL_ID)\n",
    "ov_qa_pipeline_ptq = pipeline(\"question-answering\", model=quantized_model_ptq, tokenizer=tokenizer)\n",
    "hf_qa_pipeline = pipeline(\"question-answering\", model=original_model, tokenizer=tokenizer)\n",
    "\n",
    "squad_eval = evaluator(\"question-answering\")\n",
    "\n",
    "ov_eval_results = squad_eval.compute(\n",
    "    model_or_pipeline=ov_qa_pipeline_ptq,\n",
    "    data=validation_examples,\n",
    "    metric=\"squad\",\n",
    "    squad_v2_format=VERSION_2_WITH_NEGATIVE,\n",
    ")\n",
    "\n",
    "hf_eval_results = squad_eval.compute(\n",
    "    model_or_pipeline=hf_qa_pipeline,\n",
    "    data=validation_examples,\n",
    "    metric=\"squad\",\n",
    "    squad_v2_format=VERSION_2_WITH_NEGATIVE,\n",
    ")\n",
    "pd.DataFrame.from_records(\n",
    "    [hf_eval_results, ov_eval_results], columns=[\"exact_match\", \"f1\"], index=[\"FP32\", \"INT8 PTQ\"]\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1b5d25-b248-4249-ab90-900b117e7ff3",
   "metadata": {},
   "source": [
    "### Compare model size\n",
    "\n",
    "Quantization reduces the size of the model by up to four times. We save the FP32 PyTorch model and define a function to show the model size for the PyTorch and OpenVINO models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eeaa81f-7fc5-49ba-80b8-2d95a1310a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model_folder, framework):\n",
    "    \"\"\"Return OpenVINO or PyTorch model size in Mb\"\"\"\n",
    "    if framework == \"openvino\":\n",
    "        model_path = Path(model_folder) / \"openvino_model.xml\"\n",
    "        model_size = model_path.stat().st_size + model_path.with_suffix(\".bin\").stat().st_size\n",
    "    elif framework == \"pytorch\":\n",
    "        model_path = Path(model_folder) / \"pytorch_model.bin\"\n",
    "        model_size = model_path.stat().st_size\n",
    "    model_size /= 1024 * 1024\n",
    "    return model_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05b11965-46b9-4dcf-88d8-dd8e08638b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3905629046695402"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(fp32_model_path)\n",
    "get_model_size(fp32_model_path, \"pytorch\") / get_model_size(int8_ptq_model_path, \"openvino\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9c568e-5a9d-4d99-b173-862e1c98b176",
   "metadata": {},
   "source": [
    "## Quantization Aware Training\n",
    "\n",
    "Post training quantization worked reasonably well, but resulted in a drop in exact_match of of a few percentage points. Quantization aware training integrates quantization in the training loop. The \"quantization error\" is added to the loss function, which reduces the accuracy drop in the resulting model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85519857-8147-426a-b70b-f150c9a5dd58",
   "metadata": {},
   "source": [
    "### Prepare data for QuestionAnsweringTrainer\n",
    "\n",
    "The QuestionAnsweringTrainer expects the data to be formatted in a specific way. We use the train and validation examples from the post training quantization example, and map them to a dataset formatted for quantization aware training.\n",
    "\n",
    "The `prepare_train_features`, `prepare_validation_features` and `post_processing_function_qa` functions were adapted from the [Question Answering example script](https://github.com/huggingface/optimum-intel/tree/main/examples/openvino/question-answering).  Please check out the [bert_utils](bert_utils.py) file to see how they are defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b464e1d-f7f2-4bef-9d54-a0b936588fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_qat = train_examples.map(\n",
    "    lambda x: prepare_train_features(x, tokenizer, True),\n",
    "    batched=True,\n",
    "    remove_columns=train_examples.column_names,\n",
    "    load_from_cache_file=True,  # not data_args.overwrite_cache,\n",
    "    desc=\"Running tokenizer on train dataset\",\n",
    ")\n",
    "\n",
    "validation_dataset_qat = validation_examples.map(\n",
    "    lambda x: prepare_validation_features(x, tokenizer, True),\n",
    "    batched=True,\n",
    "    remove_columns=validation_examples.column_names,\n",
    "    load_from_cache_file=True,  # not data_args.overwrite_cache,\n",
    "    desc=\"Running tokenizer on validation dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372a1c2d-a719-4149-92f0-65c3e4a12cfd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Quantize the Model with Quantization Aware Training\n",
    "\n",
    "For quantization aware training, we create a QuestionAnsweringTrainer. This Trainer is defined in [trainer_qa.py](trainer_qa.py) and taken from the [Question Answering example](https://github.com/huggingface/optimum-intel/tree/main/examples/openvino/question-answering). It is a modified version of the standard Hugging Face [QuestionAnsweringTrainer](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) that adds quantization aware training with [NNCF](https://github.com/openvinotoolkit/nncf/). See the Hugging Face [Trainer documentation](https://huggingface.co/docs/transformers/main_classes/trainer) for more information on the Trainer class.\n",
    "\n",
    "Apart from the standard training arguments, the QuestionAnsweringTrainer for NNCF requires an `ov_config` parameter with quantization settings. The default `OVConfig()` settings should work well for many cases. For more information about modifying the settings, or to understand what the settings mean, please refer to the [NNCF quantization documentation](https://github.com/openvinotoolkit/nncf/blob/develop/docs/compression_algorithms/Quantization.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1afcc609-c0fe-4739-89f6-1c798cf74518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'quantization',\n",
       " 'preset': 'mixed',\n",
       " 'overflow_fix': 'disable',\n",
       " 'initializer': {'range': {'num_init_samples': 300, 'type': 'mean_min_max'},\n",
       "  'batchnorm_adaptation': {'num_bn_adaptation_samples': 0}},\n",
       " 'scope_overrides': {'activations': {'{re}.*matmul_0': {'mode': 'symmetric'}}},\n",
       " 'ignored_scopes': ['{re}.*Embeddings.*',\n",
       "  '{re}.*__add___[0-1]',\n",
       "  '{re}.*layer_norm_0',\n",
       "  '{re}.*matmul_1',\n",
       "  '{re}.*__truediv__*'],\n",
       " 'export_to_onnx_standard_ops': False}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the quantization configuration\n",
    "OVConfig().compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f6d8449-663a-4278-b33d-46a9c80958ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in models/csarron/bert-base-uncased-squad-v1_INT8_QAT/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 362.7384, 'train_samples_per_second': 3.418, 'train_steps_per_second': 0.43, 'train_loss': 0.7960657951159354, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in models/csarron/bert-base-uncased-squad-v1_INT8_QAT/pytorch_model.bin\n",
      "tokenizer config file saved in models/csarron/bert-base-uncased-squad-v1_INT8_QAT/tokenizer_config.json\n",
      "Special tokens file saved in models/csarron/bert-base-uncased-squad-v1_INT8_QAT/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    \"\"\"Helper function for metric computation in training loop\"\"\"\n",
    "    return metric.compute(predictions=p.predictions, references=p.label_ids)\n",
    "\n",
    "\n",
    "metric = evaluate.load(\"squad_v2\" if VERSION_2_WITH_NEGATIVE else \"squad\")\n",
    "ov_config = OVConfig()\n",
    "ov_config.compression[\"overflow_fix\"] = \"disable\"\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_ID)\n",
    "\n",
    "\n",
    "trainer = QuestionAnsweringTrainer(\n",
    "    model=model,\n",
    "    ov_config=ov_config,\n",
    "    feature=\"question-answering\",\n",
    "    args=TrainingArguments(int8_qat_model_path, num_train_epochs=2.0, do_train=True, do_eval=True),\n",
    "    train_dataset=train_dataset_qat,\n",
    "    eval_dataset=validation_dataset_qat,\n",
    "    eval_examples=validation_examples,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    post_process_function=post_processing_function_qa,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "train_result = trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eabede-7177-4cfe-847d-8b9102226e4e",
   "metadata": {},
   "source": [
    "### Show accuracy difference\n",
    "\n",
    "We use the same evaluator as we did for Post Training Quantization, and show the results of all three models (FP32, INT8 PTQ and INT8 QAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e96f078-2744-430e-9057-ed381238c9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models/csarron/bert-base-uncased-squad-v1_INT8_QAT/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"models/csarron/bert-base-uncased-squad-v1_INT8_QAT/config.json\",\n",
      "  \"architectures\": [\n",
      "    \"NNCFNetwork\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exact_match</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FP32</th>\n",
       "      <td>82.86</td>\n",
       "      <td>86.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INT8 PTQ</th>\n",
       "      <td>82.86</td>\n",
       "      <td>87.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INT QAT</th>\n",
       "      <td>79.05</td>\n",
       "      <td>83.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          exact_match     f1\n",
       "FP32            82.86  86.33\n",
       "INT8 PTQ        82.86  87.42\n",
       "INT QAT         79.05  83.40"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_eval = evaluator(\"question-answering\")\n",
    "\n",
    "quantized_model_qat = OVModelForQuestionAnswering.from_pretrained(int8_qat_model_path)\n",
    "ov_qa_pipeline_qat = pipeline(\"question-answering\", model=quantized_model_qat, tokenizer=tokenizer)\n",
    "ov_eval_results_qat = squad_eval.compute(\n",
    "    model_or_pipeline=ov_qa_pipeline_qat,\n",
    "    data=validation_examples,\n",
    "    metric=\"squad\",\n",
    "    squad_v2_format=VERSION_2_WITH_NEGATIVE,\n",
    ")\n",
    "eval_results = [hf_eval_results, ov_eval_results, ov_eval_results_qat]\n",
    "df = pd.DataFrame.from_records(eval_results, columns=[\"exact_match\", \"f1\"], index=[\"FP32\", \"INT8 PTQ\", \"INT QAT\"])\n",
    "df.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c14dc07-086e-435e-bf3d-e74f1f6860cf",
   "metadata": {},
   "source": [
    "## Show inference\n",
    "\n",
    "Hugging Face `pipeline`'s simplify inference on a model. A pipeline is created by adding a task, model and tokenizer to the `pipeline` function. Inference is then as simple as `qa_pipeline(\"question\", \"context\").\n",
    "\n",
    "We created three pipelines earlier in this notebook: `hf_qa_pipeline`, `ov_qa_pipeline_ptq` and `ov_qa_pipeline_qat` for the FP32 Hugging Face model and the INT8 PTQ and QAT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e23fe96-8d7f-4aa1-816f-707ca1a2f978",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Super Bowl 50 featured numerous records from individuals and teams. Denver won despite being massively outgained in total yards (315 to 194) and first downs (21 to 11). Their 194 yards and 11 first downs were both the lowest totals ever by a Super Bowl winning team. The previous record was 244 yards by the Baltimore Ravens in Super Bowl XXXV. Only seven other teams had ever gained less than 200 yards in a Super Bowl, and all of them had lost. The Broncos' seven sacks tied a Super Bowl record set by the Chicago Bears in Super Bowl XX. Kony Ealy tied a Super Bowl record with three sacks. Jordan Norwood's 61-yard punt return set a new record, surpassing the old record of 45 yards set by John Taylor in Super Bowl XXIII. Denver was just 1-of-14 on third down, while Carolina was barely better at 3-of-15. The two teams' combined third down conversion percentage of 13.8 was a Super Bowl low. Manning and Newton had quarterback passer ratings of 56.6 and 55.4, respectively, and their added total of 112 is a record lowest aggregate passer rating for a Super Bowl. Manning became the oldest quarterback ever to win a Super Bowl at age 39, and the first quarterback ever to win a Super Bowl with two different teams, while Gary Kubiak became the first head coach to win a Super Bowl with the same franchise he went to the Super Bowl with as a player.\n"
     ]
    }
   ],
   "source": [
    "context = validation_examples[200][\"context\"]\n",
    "question = \"Who won the game?\"\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1168f1c-14de-4aad-977d-122a8d366935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Denver'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_qa_pipeline(question, context)[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c885d378-2842-49d0-b583-a2fc023558b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Denver'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ov_qa_pipeline_ptq(question, context)[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d6e0a25-9282-4b54-b42a-faad17c478a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Denver'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ov_qa_pipeline_qat(question, context)[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db183795-6dae-4ef6-847d-042223264149",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-07T21:25:39.912874Z",
     "iopub.status.busy": "2022-11-07T21:25:39.912662Z",
     "iopub.status.idle": "2022-11-07T21:25:39.916029Z",
     "shell.execute_reply": "2022-11-07T21:25:39.915541Z",
     "shell.execute_reply.started": "2022-11-07T21:25:39.912859Z"
    }
   },
   "source": [
    "## Compare Inference of FP32 and INT8 models\n",
    "\n",
    "Metrics like exact match and F1 score give an impression of the quality of the model, but to get a better sense of the quality of the model, it's always useful to look at model predictions. In the next cell, we go over the items in the validation set, and display the items where the FP32 prediction score is different than the INT8 prediction score. In this example we compare the FP32 model with the QAT model; it can also be insightful to compare the PTQ model and the QAT model.\n",
    "\n",
    "The results show that for some predictions, the FP32 model is better, but for others, the INT8 model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "256e4b8c-a791-4668-95e3-4d23720fcf94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>FP32 answer</th>\n",
       "      <th>FP32 F1</th>\n",
       "      <th>INT8 answer</th>\n",
       "      <th>INT8 F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which company won a contest to have their ad shown for free during Super Bowl 50?</td>\n",
       "      <td>[Death Wish Coffee, Death Wish Coffee, Death Wish Coffee]</td>\n",
       "      <td>QuickBooks</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Death Wish Coffee</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What company paid for a Super Bowl 50 ad to show a trailer of X-Men: Apocalypse?</td>\n",
       "      <td>[Fox, Fox, Disney]</td>\n",
       "      <td>20th Century Fox, Lionsgate</td>\n",
       "      <td>40.00</td>\n",
       "      <td>Paramount Pictures, Universal Studios and Walt Disney Studios</td>\n",
       "      <td>22.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who handled the play-by-play for WBT?</td>\n",
       "      <td>[Mick Mixon, Mick Mixon, Mick Mixon]</td>\n",
       "      <td>Mick Mixon</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Dave Logan</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How many of the prior Super Bowl MVPs appeared together at the pregame show?</td>\n",
       "      <td>[39, 39, 39]</td>\n",
       "      <td>39</td>\n",
       "      <td>100.00</td>\n",
       "      <td>39 of the 43</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>At what Super Bowl did Beyoncé headline the halftime show?</td>\n",
       "      <td>[Super Bowl XLVII, Super Bowl XLVII, XLVII]</td>\n",
       "      <td>Super Bowl XLVII</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Super Bowl XLVII halftime show</td>\n",
       "      <td>75.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What previous Super Bowl halftime show did Bruno Mars headline?</td>\n",
       "      <td>[Super Bowl XLVIII, Super Bowl XLVIII, XLVIII]</td>\n",
       "      <td>Super Bowl XLVIII</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Super Bowl XLVIII halftime show</td>\n",
       "      <td>75.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Who was at the receiving end of a 22-yard pass from Peyton Manning?</td>\n",
       "      <td>[Andre Caldwell, Andre Caldwell, Caldwell]</td>\n",
       "      <td>Andre Caldwell</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Owen Daniels</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How many yards was the pass on the first drive?</td>\n",
       "      <td>[18, 18, 22-yard]</td>\n",
       "      <td>18</td>\n",
       "      <td>100.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What year was the last time a fumble return touchdown like this occurred?</td>\n",
       "      <td>[1993, 1993, 1993]</td>\n",
       "      <td>1993</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1993 season.</td>\n",
       "      <td>66.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How many passing yards did Cam Newton get for his 4 of 4 passes?</td>\n",
       "      <td>[51, 51, 51]</td>\n",
       "      <td>51</td>\n",
       "      <td>100.00</td>\n",
       "      <td>51 yards</td>\n",
       "      <td>66.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Who got a 61-yard return when players thought he called for a fair catch but did not?</td>\n",
       "      <td>[Jordan Norwood, Jordan Norwood, Norwood]</td>\n",
       "      <td>Jordan Norwood</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Mario Addison</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How many yards was the field goal that made the score 13-7 in Super Bowl 50?</td>\n",
       "      <td>[33, 33, 33]</td>\n",
       "      <td>33</td>\n",
       "      <td>100.00</td>\n",
       "      <td>10–7</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How did the drive end for the Panthers?</td>\n",
       "      <td>[punt, Newton was sacked, sacked]</td>\n",
       "      <td>The Panthers could not gain any yards with their possession and had to punt</td>\n",
       "      <td>14.29</td>\n",
       "      <td>punt</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Who picked off Cam Newton and subsequently fumbled the ball?</td>\n",
       "      <td>[T. J. Ward, T. J. Ward, Ward]</td>\n",
       "      <td>Trevathan</td>\n",
       "      <td>0.00</td>\n",
       "      <td>T. J. Ward</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What yard line was the Broncos on when Manning lost the ball in the fourth quarter?</td>\n",
       "      <td>[50-yard line., 41, 50]</td>\n",
       "      <td>41</td>\n",
       "      <td>100.00</td>\n",
       "      <td>41-yard line</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>On what yard line did Carolina begin with 4:51 left in the game?</td>\n",
       "      <td>[24, their own 24, 24]</td>\n",
       "      <td>24</td>\n",
       "      <td>100.00</td>\n",
       "      <td>24-yard line</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Who fumbled the ball on 3rd-and-9?</td>\n",
       "      <td>[Newton, Newton, Newton]</td>\n",
       "      <td>Miller stripped the ball away from Newton</td>\n",
       "      <td>28.57</td>\n",
       "      <td>Miller</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How many interceptions did Manning have at the end of the game?</td>\n",
       "      <td>[one, one, one]</td>\n",
       "      <td>one</td>\n",
       "      <td>100.00</td>\n",
       "      <td>13 of 23 for 141 yards with one</td>\n",
       "      <td>22.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How many of the four field goal attempts did McManus succeed at during SUper Bowl 50?</td>\n",
       "      <td>[all four, all four, four]</td>\n",
       "      <td>11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>all four</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How many first downs did Denver have?</td>\n",
       "      <td>[11, 11, 11]</td>\n",
       "      <td>21 to 11</td>\n",
       "      <td>50.00</td>\n",
       "      <td>11</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>How many first downs did Denver have for Super Bowl 50?</td>\n",
       "      <td>[11, 11, 11]</td>\n",
       "      <td>21 to 11</td>\n",
       "      <td>50.00</td>\n",
       "      <td>11</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Who did the Broncos tie with the most sacks in a Super Bowl?</td>\n",
       "      <td>[Chicago Bears, the Chicago Bears, Bears]</td>\n",
       "      <td>Chicago Bears</td>\n",
       "      <td>100.00</td>\n",
       "      <td>Chicago Bears in Super Bowl XX. Kony Ealy</td>\n",
       "      <td>40.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>How many first downs did the Panthers have in Super Bowl 50?</td>\n",
       "      <td>[21, 21, 21]</td>\n",
       "      <td>21 to 11</td>\n",
       "      <td>50.00</td>\n",
       "      <td>11</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>How many first downs did the Broncos have in Super Bowl 50?</td>\n",
       "      <td>[11, 11, 11]</td>\n",
       "      <td>21 to 11</td>\n",
       "      <td>50.00</td>\n",
       "      <td>11</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>How many teams has Manning won the Super Bowl with?</td>\n",
       "      <td>[two, two, two]</td>\n",
       "      <td>two</td>\n",
       "      <td>100.00</td>\n",
       "      <td>seven</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                 Question  \\\n",
       "0       Which company won a contest to have their ad shown for free during Super Bowl 50?   \n",
       "1        What company paid for a Super Bowl 50 ad to show a trailer of X-Men: Apocalypse?   \n",
       "2                                                   Who handled the play-by-play for WBT?   \n",
       "3            How many of the prior Super Bowl MVPs appeared together at the pregame show?   \n",
       "4                              At what Super Bowl did Beyoncé headline the halftime show?   \n",
       "5                         What previous Super Bowl halftime show did Bruno Mars headline?   \n",
       "6                     Who was at the receiving end of a 22-yard pass from Peyton Manning?   \n",
       "7                                         How many yards was the pass on the first drive?   \n",
       "8               What year was the last time a fumble return touchdown like this occurred?   \n",
       "9                        How many passing yards did Cam Newton get for his 4 of 4 passes?   \n",
       "10  Who got a 61-yard return when players thought he called for a fair catch but did not?   \n",
       "11           How many yards was the field goal that made the score 13-7 in Super Bowl 50?   \n",
       "12                                                How did the drive end for the Panthers?   \n",
       "13                           Who picked off Cam Newton and subsequently fumbled the ball?   \n",
       "14    What yard line was the Broncos on when Manning lost the ball in the fourth quarter?   \n",
       "15                       On what yard line did Carolina begin with 4:51 left in the game?   \n",
       "16                                                     Who fumbled the ball on 3rd-and-9?   \n",
       "17                        How many interceptions did Manning have at the end of the game?   \n",
       "18  How many of the four field goal attempts did McManus succeed at during SUper Bowl 50?   \n",
       "19                                                  How many first downs did Denver have?   \n",
       "20                                How many first downs did Denver have for Super Bowl 50?   \n",
       "21                           Who did the Broncos tie with the most sacks in a Super Bowl?   \n",
       "22                           How many first downs did the Panthers have in Super Bowl 50?   \n",
       "23                            How many first downs did the Broncos have in Super Bowl 50?   \n",
       "24                                    How many teams has Manning won the Super Bowl with?   \n",
       "\n",
       "                                                       Answer  \\\n",
       "0   [Death Wish Coffee, Death Wish Coffee, Death Wish Coffee]   \n",
       "1                                          [Fox, Fox, Disney]   \n",
       "2                        [Mick Mixon, Mick Mixon, Mick Mixon]   \n",
       "3                                                [39, 39, 39]   \n",
       "4                 [Super Bowl XLVII, Super Bowl XLVII, XLVII]   \n",
       "5              [Super Bowl XLVIII, Super Bowl XLVIII, XLVIII]   \n",
       "6                  [Andre Caldwell, Andre Caldwell, Caldwell]   \n",
       "7                                           [18, 18, 22-yard]   \n",
       "8                                          [1993, 1993, 1993]   \n",
       "9                                                [51, 51, 51]   \n",
       "10                  [Jordan Norwood, Jordan Norwood, Norwood]   \n",
       "11                                               [33, 33, 33]   \n",
       "12                          [punt, Newton was sacked, sacked]   \n",
       "13                             [T. J. Ward, T. J. Ward, Ward]   \n",
       "14                                    [50-yard line., 41, 50]   \n",
       "15                                     [24, their own 24, 24]   \n",
       "16                                   [Newton, Newton, Newton]   \n",
       "17                                            [one, one, one]   \n",
       "18                                 [all four, all four, four]   \n",
       "19                                               [11, 11, 11]   \n",
       "20                                               [11, 11, 11]   \n",
       "21                  [Chicago Bears, the Chicago Bears, Bears]   \n",
       "22                                               [21, 21, 21]   \n",
       "23                                               [11, 11, 11]   \n",
       "24                                            [two, two, two]   \n",
       "\n",
       "                                                                    FP32 answer  \\\n",
       "0                                                                    QuickBooks   \n",
       "1                                                   20th Century Fox, Lionsgate   \n",
       "2                                                                    Mick Mixon   \n",
       "3                                                                            39   \n",
       "4                                                              Super Bowl XLVII   \n",
       "5                                                             Super Bowl XLVIII   \n",
       "6                                                                Andre Caldwell   \n",
       "7                                                                            18   \n",
       "8                                                                          1993   \n",
       "9                                                                            51   \n",
       "10                                                               Jordan Norwood   \n",
       "11                                                                           33   \n",
       "12  The Panthers could not gain any yards with their possession and had to punt   \n",
       "13                                                                    Trevathan   \n",
       "14                                                                           41   \n",
       "15                                                                           24   \n",
       "16                                    Miller stripped the ball away from Newton   \n",
       "17                                                                          one   \n",
       "18                                                                           11   \n",
       "19                                                                     21 to 11   \n",
       "20                                                                     21 to 11   \n",
       "21                                                                Chicago Bears   \n",
       "22                                                                     21 to 11   \n",
       "23                                                                     21 to 11   \n",
       "24                                                                          two   \n",
       "\n",
       "    FP32 F1                                                    INT8 answer  \\\n",
       "0      0.00                                              Death Wish Coffee   \n",
       "1     40.00  Paramount Pictures, Universal Studios and Walt Disney Studios   \n",
       "2    100.00                                                     Dave Logan   \n",
       "3    100.00                                                   39 of the 43   \n",
       "4    100.00                                 Super Bowl XLVII halftime show   \n",
       "5    100.00                                Super Bowl XLVIII halftime show   \n",
       "6    100.00                                                   Owen Daniels   \n",
       "7    100.00                                                             20   \n",
       "8    100.00                                                   1993 season.   \n",
       "9    100.00                                                       51 yards   \n",
       "10   100.00                                                  Mario Addison   \n",
       "11   100.00                                                           10–7   \n",
       "12    14.29                                                           punt   \n",
       "13     0.00                                                     T. J. Ward   \n",
       "14   100.00                                                   41-yard line   \n",
       "15   100.00                                                   24-yard line   \n",
       "16    28.57                                                         Miller   \n",
       "17   100.00                                13 of 23 for 141 yards with one   \n",
       "18     0.00                                                       all four   \n",
       "19    50.00                                                             11   \n",
       "20    50.00                                                             11   \n",
       "21   100.00                      Chicago Bears in Super Bowl XX. Kony Ealy   \n",
       "22    50.00                                                             11   \n",
       "23    50.00                                                             11   \n",
       "24   100.00                                                          seven   \n",
       "\n",
       "    INT8 F1  \n",
       "0    100.00  \n",
       "1     22.22  \n",
       "2      0.00  \n",
       "3     50.00  \n",
       "4     75.00  \n",
       "5     75.00  \n",
       "6      0.00  \n",
       "7      0.00  \n",
       "8     66.67  \n",
       "9     66.67  \n",
       "10     0.00  \n",
       "11     0.00  \n",
       "12   100.00  \n",
       "13   100.00  \n",
       "14    50.00  \n",
       "15     0.00  \n",
       "16     0.00  \n",
       "17    22.22  \n",
       "18   100.00  \n",
       "19   100.00  \n",
       "20   100.00  \n",
       "21    40.00  \n",
       "22     0.00  \n",
       "23   100.00  \n",
       "24     0.00  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for item in validation_examples:\n",
    "    id, title, context, question, answers = item.values()\n",
    "    fp32_answer = hf_qa_pipeline(question, context)[\"answer\"]\n",
    "    int8_answer = ov_qa_pipeline_qat(question, context)[\"answer\"]\n",
    "\n",
    "    references = [{\"id\": id, \"answers\": answers}]\n",
    "    fp32_predictions = [{\"id\": id, \"prediction_text\": fp32_answer}]\n",
    "    int8_predictions = [{\"id\": id, \"prediction_text\": int8_answer}]\n",
    "\n",
    "    fp32_score = round(metric.compute(references=references, predictions=fp32_predictions)[\"f1\"], 2)\n",
    "    int8_score = round(metric.compute(references=references, predictions=int8_predictions)[\"f1\"], 2)\n",
    "\n",
    "    if int8_score != fp32_score:\n",
    "        results.append((question, answers[\"text\"], fp32_answer, fp32_score, int8_answer, int8_score))\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.DataFrame(\n",
    "    results,\n",
    "    columns=[\"Question\", \"Answer\", \"FP32 answer\", \"FP32 F1\", \"INT8 answer\", \"INT8 F1\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8697a2-0a51-427f-8245-cda56bb8cf18",
   "metadata": {},
   "source": [
    "## Benchmark the Quantized and Original Model\n",
    "\n",
    "Compare the inference speed of the quantized OpenVINO IR model with that of the original PyTorch model.\n",
    "\n",
    "OpenVINO models can optionally be used with static shapes, which increases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8806da79-0b3b-403e-a40c-61db6a0f482d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models/csarron/bert-base-uncased-squad-v1_INT8_QAT/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"models/csarron/bert-base-uncased-squad-v1_INT8_QAT/config.json\",\n",
      "  \"architectures\": [\n",
      "    \"NNCFNetwork\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--csarron--bert-base-uncased-squad-v1/snapshots/a39235c4e278ec8b420b46ee11a1ed1a432a7256/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"csarron/bert-base-uncased-squad-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--csarron--bert-base-uncased-squad-v1/snapshots/a39235c4e278ec8b420b46ee11a1ed1a432a7256/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "All the weights of BertForQuestionAnswering were initialized from the model checkpoint at csarron/bert-base-uncased-squad-v1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency of original FP32 model: 38.13 ms\n",
      "Latency of quantized model: 16.89 ms\n",
      "Speedup: 2.26x\n"
     ]
    }
   ],
   "source": [
    "def benchmark(model, static_shapes=True):\n",
    "    \"\"\" \"\"\"\n",
    "    transformers.logging.set_verbosity_error()\n",
    "\n",
    "    kwargs = {}\n",
    "    if static_shapes:\n",
    "        if model.base_model_prefix == \"openvino\":\n",
    "            model.reshape(1, 256)\n",
    "            model.compile()\n",
    "        kwargs = {\"max_seq_len\": 256, \"padding\": \"max_length\", \"truncation\": True}\n",
    "\n",
    "    qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, **kwargs)\n",
    "\n",
    "    ds = datasets.load_dataset(\"squad\", split=\"validation[:300]\")\n",
    "    latencies = []\n",
    "    for i, item in enumerate(ds):\n",
    "        start_time = time.perf_counter()\n",
    "        results = qa_pipeline({\"question\": item[\"question\"], \"context\": item[\"context\"]})\n",
    "        end_time = time.perf_counter()\n",
    "        latencies.append(end_time - start_time)\n",
    "\n",
    "    return np.median(latencies) * 1000\n",
    "\n",
    "\n",
    "quantized_model = OVModelForQuestionAnswering.from_pretrained(int8_qat_model_path)\n",
    "original_model = AutoModelForQuestionAnswering.from_pretrained(MODEL_ID)\n",
    "# original_model = OVModelForQuestionAnswering.from_pretrained(\"bert-base-fp32\")\n",
    "\n",
    "\n",
    "original_latency = benchmark(original_model, static_shapes=True)\n",
    "quantized_latency = benchmark(quantized_model, static_shapes=True)\n",
    "\n",
    "print(f\"Latency of original FP32 model: {original_latency:.2f} ms\")\n",
    "print(f\"Latency of quantized model: {quantized_latency:.2f} ms\")\n",
    "print(f\"Speedup: {(original_latency/quantized_latency):.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d64ac5a-fed2-40f6-9ef4-d1ce36e81c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency of original FP32 model: 27.08 ms\n",
      "Latency of quantized model: 23.59 ms\n",
      "Speedup: 1.15x\n"
     ]
    }
   ],
   "source": [
    "original_latency = benchmark(original_model, static_shapes=False)\n",
    "quantized_latency = benchmark(quantized_model, static_shapes=False)\n",
    "\n",
    "print(f\"Latency of original FP32 model: {original_latency:.2f} ms\")\n",
    "print(f\"Latency of quantized model: {quantized_latency:.2f} ms\")\n",
    "print(f\"Speedup: {(original_latency/quantized_latency):.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b71a9850-ae2e-4040-ad95-5bfe13a2d01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'disable'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ov_config.compression[\"overflow_fix\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59bb2986-7518-418f-8bd1-1002893bc287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.3.0', '2022.3.0-8831-4f0b846d1a5')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nncf\n",
    "from openvino.runtime import get_version\n",
    "nncf.__version__ , get_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "559efc67-99c0-4ac5-96e9-36bf0d326143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: nncf\n",
      "Version: 2.3.0.dev0+f1d8c266\n",
      "Summary: Neural Networks Compression Framework\n",
      "Home-page: https://github.com/openvinotoolkit/nncf\n",
      "Author: Intel\n",
      "Author-email: alexander.kozlov@intel.com\n",
      "License: UNKNOWN\n",
      "Location: /home/ubuntu/venvs/optimum_pre_env/lib/python3.8/site-packages\n",
      "Requires: addict, jsonschema, jstyleson, matplotlib, natsort, networkx, ninja, numpy, openvino-telemetry, pandas, pillow, pydot, pymoo, pyparsing, scikit-learn, scipy, texttable, tqdm, wheel\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show nncf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94e273f4-a753-40cc-ad3f-e8b8dacc56b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openvino\n",
      "Version: 2022.3.0.dev20221125\n",
      "Summary: OpenVINO(TM) Runtime\n",
      "Home-page: https://docs.openvino.ai/nightly/index.html\n",
      "Author: Intel Corporation\n",
      "Author-email: openvino_pushbot@intel.com\n",
      "License: OSI Approved :: Apache Software License\n",
      "Location: /home/ubuntu/venvs/optimum_pre_env/lib/python3.8/site-packages\n",
      "Requires: numpy\n",
      "Required-by: openvino-dev\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show openvino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54ebe2-1fcf-4d98-8ab5-db6b5d0d871a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
