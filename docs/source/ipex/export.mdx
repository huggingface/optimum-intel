<!--Copyright 2024 The HuggingFace Team. All rights reserved.
Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Export your model

## Using the CLI

To export your model to the IPEX optimized (patching + weight repack + jit trace) model with the CLI :

```bash
optimum-cli export ipex -m gpt2 --torch_dtype bfloat16 ipex-gpt2
```

The model argument can either be the model ID of a model hosted on the [Hub](https://huggingface.co/models) or a path to a model hosted locally. For local models, you need to specify the task for which the model should be loaded before export, among the list of the [supported tasks](https://github.com/huggingface/optimum-intel/blob/main/optimum/intel/ipex/utils.py).

```bash
optimum-cli export ipex -m local_gpt2 --task text-generation --torch_dtype bfloat16 ipex-gpt2
```

Check out the help for more options:

```bash
optimum-cli export ipex --help

usage: optimum-cli export ipex [-h] -m MODEL [--task TASK] [--trust_remote_code] [--revision REVISION] [--token TOKEN] [--cache_dir CACHE_DIR] [--subfolder SUBFOLDER]
                               [--local_files_only LOCAL_FILES_ONLY] [--force_download FORCE_DOWNLOAD] [--commit_hash COMMIT_HASH] [--torch_dtype TORCH_DTYPE]
                               output

options:
  -h, --help            show this help message and exit

Required arguments:
  -m MODEL, --model MODEL
                        Model ID on huggingface.co or path on disk to load model from.
  output                Path indicating the directory where to store the generated IPEX model.

Optional arguments:
  --task TASK           The task to export the model for. If not specified, the task will be auto-inferred based on the model. Available tasks depend on the model.
  --trust_remote_code   Allows to use custom code for the modeling hosted in the model repository. This option should only be set for repositories you trust and in which you have read the code, as it
                        will execute on your local machine arbitrary code present in the model repository.
  --revision REVISION   The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models and other artifacts on huggingface.co,
                        so `revision` can be any identifier allowed by git.
  --token TOKEN         The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated when running `huggingface-cli login` (stored in
                        `huggingface_hub.constants.HF_TOKEN_PATH`).
  --cache_dir CACHE_DIR
                        Path to a directory in which a downloaded pretrained model configuration should be cached if the standard cache should not be used.
  --subfolder SUBFOLDER
                        In case the relevant files are located inside a subfolder of the model repo either locally or on huggingface.co, you can specify the folder name here.
  --local_files_only LOCAL_FILES_ONLY
                        Whether or not to only look at local files (i.e., do not try to download the model).
  --force_download FORCE_DOWNLOAD
                        Whether or not to force the (re-)download of the model weights and configuration files, overriding the cached versions if they exist.
  --commit_hash COMMIT_HASH
                        The commit_hash related to the file.
  --torch_dtype TORCH_DTYPE
                        float16 or bfloat16 or float32: load in a specified dtype, ignoring the modelâ€™s config.torch_dtype if one exists. If not specified, the model will get loaded in float32.
```
