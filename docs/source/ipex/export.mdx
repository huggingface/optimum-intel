<!--Copyright 2024 The HuggingFace Team. All rights reserved.
Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Export your model

## Using the CLI

To export your model to the IPEX format with the CLI :

```bash
optimum-cli export ipex -m gpt2 --torch_dtype bfloat16 ipex-gpt2
```

The model argument can either be the model ID of a model hosted on the [Hub](https://huggingface.co/models) or a path to a model hosted locally. For local models, you need to specify the task for which the model should be loaded before export, among the list of the [supported tasks](https://github.com/huggingface/optimum-intel/blob/main/optimum/intel/ipex/utils.py).

```bash
optimum-cli export ipex -m local_gpt2 --task text-generation --torch_dtype bfloat16 ipex-gpt2
```

Check out the help for more options:

```bash
optimum-cli export ipex --help

usage: optimum-cli export ipex [-h] -m MODEL [--task TASK] [--trust_remote_code] [--revision REVISION] [--token TOKEN] [--cache_dir CACHE_DIR] [--subfolder SUBFOLDER]
                               [--local_files_only LOCAL_FILES_ONLY] [--force_download FORCE_DOWNLOAD] [--commit_hash COMMIT_HASH] [--torch_dtype TORCH_DTYPE]
                               output

options:
  -h, --help            show this help message and exit

Required arguments:
  -m MODEL, --model MODEL
                        Model ID on huggingface.co or path on disk to load model from.
  output                Path indicating the directory where to store the generated IPEX model.

Optional arguments:
  --task TASK           The task to export the model for. If not specified, the task will be auto-inferred based on the model. Available tasks depend on the model.
  --trust_remote_code   Allows to use custom code for the modeling hosted in the model repository. This option should only be set for repositories you trust and in which you have read the code, as it
                        will execute on your local machine arbitrary code present in the model repository.
  --revision REVISION   model kwargs
  --token TOKEN         model kwargs
  --cache_dir CACHE_DIR
                        model kwargs
  --subfolder SUBFOLDER
                        model kwargs
  --local_files_only LOCAL_FILES_ONLY
                        model kwargs
  --force_download FORCE_DOWNLOAD
                        model kwargs
  --commit_hash COMMIT_HASH
                        model kwargs
  --torch_dtype TORCH_DTYPE
                        model kwargs
```
